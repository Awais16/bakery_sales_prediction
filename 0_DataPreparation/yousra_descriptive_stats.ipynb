{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4030697c",
   "metadata": {},
   "source": [
    "# ðŸ“˜ Data Merging & Cleaning â€“ Yousra Descriptive Stats Notebook\n",
    "\n",
    "This notebook loads the original project datasets (sales, weather, kiwo event),  \n",
    "cleans and standardizes the date format, merges them using a full outer join,  \n",
    "and performs descriptive statistics, missing value inspection, and prepares the  \n",
    "data for further feature engineering and modeling.\n",
    "\n",
    "The goal is to:\n",
    "1. Combine **all available data** on matching dates  \n",
    "2. Perform a clear descriptive exploration  \n",
    "3. Detect and handle missing values  \n",
    "4. Produce a clean dataset ready for analysis  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f96195c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data from your project folder\n",
    "df_kiwo = pd.read_csv(\"../data/kiwo.csv\")\n",
    "df_weather = pd.read_csv(\"../data/wetter.csv\")\n",
    "df_sales = pd.read_csv(\"../data/umsatzdaten_gekuerzt.csv\")\n",
    "df_test = pd. read_csv(\"../data/test.csv\")\n",
    "\n",
    "#df_kiwo.head(), df_weather.head(), df_sales.head()\n",
    "#print number of rows for each dataframe\n",
    "print(f\"Kiwo Data Rows: {len(df_kiwo)}\")\n",
    "print(f\"Weather Data Rows: {len(df_weather)}\")\n",
    "print(f\"sales Data Rows: {len(df_sales)}\")\n",
    "print(f\"Test data frame Rows:{len(df_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360cb1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Datum' to proper datetime format and drop invalid rows\n",
    "for df in (df_kiwo, df_weather, df_sales, df_test):\n",
    "    df[\"Datum\"] = pd.to_datetime(df[\"Datum\"], errors=\"coerce\").dt.normalize()\n",
    "    df.dropna(subset=[\"Datum\"], inplace=True)\n",
    "\n",
    "    # Drop duplicate date columns if exist\n",
    "    if \"date\" in df.columns:\n",
    "        df.drop(columns=[\"date\"], inplace=True)\n",
    "df_test.info(), df_weather.info(), df_sales.info(),df_kiwo.info(),\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440b1e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding suffixes helps avoid confusion during merging\n",
    "\n",
    "df_kiwo = df_kiwo.add_suffix(\"_kiwo\")\n",
    "df_kiwo.rename(columns={\"Datum_kiwo\": \"Datum\"}, inplace=True)\n",
    "\n",
    "df_weather = df_weather.add_suffix(\"_weather\")\n",
    "df_weather.rename(columns={\"Datum_weather\": \"Datum\"}, inplace=True)\n",
    "\n",
    "\n",
    "df_sales = df_sales.add_suffix(\"_umsatz\")\n",
    "df_sales.rename(columns={\"Datum_umsatz\": \"Datum\"}, inplace=True)\n",
    "\n",
    "df_sales[\"umsatz_rolling7\"] = (\n",
    "    df_sales[\"Umsatz_umsatz\"].rolling(window=7, min_periods=1).mean()\n",
    ")\n",
    "\n",
    "# add suffix to df_test columns\n",
    "df_test = df_test.add_suffix(\"_test\")\n",
    "df_test.rename(columns={\"Datum_test\": \"Datum\"}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa7a234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print start and end dates for each dataframe\n",
    "for name, df in [(\"Kiwo\", df_kiwo), (\"Weather\", df_weather), (\"Sales\", df_sales), (\"Test\", df_test)]:\n",
    "    print(\n",
    "        f\"{name} Data: Start Date = {df['Datum'].min().date()}, End Date = {df['Datum'].max().date()}\"\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846ea610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate descriptive statistics for each dataframe\n",
    "for name, df in [(\"Weather\", df_weather), (\"Sales\", df_sales),(\"Kiwo\", df_kiwo)]:\n",
    "    print(f\"\\nDescriptive Statistics for {name} Data:\")\n",
    "    print(df.describe(include=\"all\"))\n",
    "\n",
    "display(df_sales.isna().sum(), df_weather.isna().sum(), df_kiwo.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d615494e",
   "metadata": {},
   "source": [
    "## Visualize dataframe for gaps\n",
    "We can now use this function for any data set in this playbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b425bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')  # Add project root to path for imports\n",
    "from utils import plot_missing_heatmap\n",
    "\n",
    "# Plot for each dataframe\n",
    "plot_missing_heatmap(df_sales, 'Sales')\n",
    "plot_missing_heatmap(df_weather, 'Weather')\n",
    "plot_missing_heatmap(df_kiwo, 'Kiwo')\n",
    "plot_missing_heatmap(df_test, 'Test DataFrame')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723f054c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging dataframes on 'Datum' using outer joins to retain all records\n",
    "merged_df = (\n",
    "    df_kiwo\n",
    "    .merge(df_weather, on=\"Datum\", how=\"outer\")\n",
    "    .merge(df_sales, on=\"Datum\", how=\"outer\")\n",
    ")\n",
    "\n",
    "# Merging test dataframe as well\n",
    "merged_df = merged_df.merge(df_test, on=\"Datum\", how=\"outer\")\n",
    "display(merged_df.shape)\n",
    "display(merged_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e3f9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_missing_heatmap(merged_df, 'Merged Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f05e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric summary\n",
    "merged_df.describe()\n",
    "\n",
    "# For all columns (including categorical)\n",
    "merged_df.describe(include=\"all\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7e66b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_count = merged_df.isna().sum()\n",
    "missing_percent = (merged_df.isna().sum() / len(merged_df)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    \"Missing Count\": missing_count,\n",
    "    \"Missing %\": missing_percent\n",
    "})\n",
    "\n",
    "missing_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471abb4d",
   "metadata": {},
   "source": [
    "### âœ”ï¸ Missing Value Strategy\n",
    "\n",
    "**Umsatz (Sales):**\n",
    "- Missing values mean the day has **no sales** or is **outside the bakery event period**.\n",
    "- These rows should be **removed**, not imputed.\n",
    "\n",
    "**Weather Variables:**\n",
    "- Weather data may be missing because not all dates have weather records.\n",
    "- Fill missing values using interpolation (numerical)  \n",
    "  and assign -1 for Wettercode to indicate â€œunknown weatherâ€.\n",
    "\n",
    "**Kiwo Event Flag:**\n",
    "- If missing â†’ fill with 0 (day outside event).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e16358a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = merged_df.copy()\n",
    "\n",
    "# Remove rows where sales are missing\n",
    "#clean_df = clean_df.dropna(subset=[\"Umsatz_umsatz\"])\n",
    "\n",
    "# Fill missing Wettercode with category -1\n",
    "#if \"Wettercode_weather\" in clean_df.columns:\n",
    "    # clean_df[\"Wettercode_weather\"] = clean_df[\"Wettercode_weather\"].fillna(-1)\n",
    "\n",
    "# Interpolate numerical weather values\n",
    "# for col in clean_df.columns:\n",
    "    # if (\"_weather\" in col) and (clean_df[col].dtype in [\"float64\", \"int64\"]):\n",
    "        # clean_df[col] = clean_df[col].interpolate()\n",
    "\n",
    "clean_df.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1951c081",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df[\"KielerWoche_kiwo\"] = clean_df[\"KielerWoche_kiwo\"].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5e5aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.isna().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2db0ae6",
   "metadata": {},
   "source": [
    "## Visualizing cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f63db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_missing_heatmap(clean_df, 'Cleaned Merged Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fcdfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop wettercode_weather column\n",
    "df_no_weather_code = clean_df.drop(columns=[\"Wettercode_weather\"])\n",
    "df_no_weather_code.sample(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d76d324",
   "metadata": {},
   "source": [
    "## Adding extra columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04f79a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_weather_code['Datum'] = pd.to_datetime(df_no_weather_code['Datum'], errors='coerce').dt.normalize()\n",
    "n_invalid = df_no_weather_code['Datum'].isna().sum()\n",
    "if n_invalid > 0:\n",
    "    print(f\"Warning: {n_invalid} rows have invalid 'Datum' and will have NaT in day columns. Sample:\")\n",
    "    display(df_no_weather_code[df_no_weather_code['Datum'].isna()].head())\n",
    "\n",
    "# Create integer and name columns for day of week\n",
    "df_no_weather_code['day_of_week'] = df_no_weather_code['Datum'].dt.weekday  # Monday=0 .. Sunday=6\n",
    "df_no_weather_code['day'] = df_no_weather_code['Datum'].dt.day_name()\n",
    "\n",
    "# also add  is_saturday and is_sunday columns\n",
    "df_no_weather_code['is_saturday'] = (df_no_weather_code['day_of_week'] == 5).astype(int)\n",
    "df_no_weather_code['is_sunday'] = (df_no_weather_code['day_of_week'] == 6).astype(int)\n",
    "\n",
    "# Optional: make 'day' categorical ordered Monday..Sunday\n",
    "ordered_days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "df_no_weather_code['day'] = pd.Categorical(df_no_weather_code['day'], categories=ordered_days, ordered=True)\n",
    "\n",
    "print(\"\\nCounts per weekday:\")\n",
    "print(df_no_weather_code['day'].value_counts().sort_index())\n",
    "df_no_weather_code.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798c754c",
   "metadata": {},
   "source": [
    "### Adding more weather data from meteo archive api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89809343",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime, date, timedelta\n",
    "import time\n",
    "from typing import Union, Iterable, Dict, List\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Simple in-memory cache for API responses\n",
    "try:\n",
    "    _OPEN_METEO_CACHE\n",
    "except NameError:\n",
    "    _OPEN_METEO_CACHE: Dict[str, Dict[str, float]] = {}\n",
    "\n",
    "def _iso_date(d: Union[str, date, datetime]) -> str:\n",
    "    if isinstance(d, str):\n",
    "        return datetime.fromisoformat(d).date().isoformat()\n",
    "    if isinstance(d, datetime):\n",
    "        return d.date().isoformat()\n",
    "    return d.isoformat()\n",
    "\n",
    "def fetch_open_meteo_daily_range(\n",
    "    start_date: Union[str, date, datetime],\n",
    "    end_date: Union[str, date, datetime],\n",
    "    latitude: float = 54.3233,\n",
    "    longitude: float = 10.1228,\n",
    "    timezone: str = \"Europe/Berlin\",\n",
    "    daily_vars: List[str] = None,\n",
    "    max_retries: int = 3,\n",
    "    timeout: float = 15.0,\n",
    "    use_cache: bool = True\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch daily historical variables from Open-Meteo archive API for a given inclusive date range.\n",
    "    By default uses Kiel coordinates (lat=54.3233, lon=10.1228).\n",
    "    daily_vars example: ['precipitation_hours', 'sunshine_duration', 'rain_sum']\n",
    "    Returns a pandas.DataFrame indexed by date (datetime.date) with columns named after daily_vars.\n",
    "    \"\"\"\n",
    "    if daily_vars is None:\n",
    "        daily_vars = ['precipitation_hours', 'sunshine_duration', 'rain_sum']\n",
    "\n",
    "    start_iso = _iso_date(start_date)\n",
    "    end_iso = _iso_date(end_date)\n",
    "    start_dt = datetime.fromisoformat(start_iso).date()\n",
    "    end_dt = datetime.fromisoformat(end_iso).date()\n",
    "    if end_dt < start_dt:\n",
    "        raise ValueError(\"end_date must be >= start_date\")\n",
    "\n",
    "    # Build cache key based on parameters and requested range\n",
    "    key = f\"{latitude:.6f}_{longitude:.6f}_{start_iso}_{end_iso}_{','.join(daily_vars)}_{timezone}\"\n",
    "    if use_cache and key in _OPEN_METEO_CACHE:\n",
    "        df = _OPEN_METEO_CACHE[key].copy()\n",
    "        return df\n",
    "\n",
    "    base_url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "    params = {\n",
    "        \"latitude\": latitude,\n",
    "        \"longitude\": longitude,\n",
    "        \"start_date\": start_iso,\n",
    "        \"end_date\": end_iso,\n",
    "        \"daily\": \",\".join(daily_vars),\n",
    "        \"timezone\": timezone\n",
    "    }\n",
    "\n",
    "    last_exc = None\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            r = requests.get(base_url, params=params, timeout=timeout)\n",
    "            r.raise_for_status()\n",
    "            data = r.json()\n",
    "            daily = data.get(\"daily\", {})\n",
    "            times = daily.get(\"time\", []) or []\n",
    "            if not times:\n",
    "                raise ValueError(\"No 'time' in API response daily block.\")\n",
    "            results = {\"date\": pd.to_datetime(times).date}\n",
    "            for v in daily_vars:\n",
    "                vals = daily.get(v, None)\n",
    "                if vals is None:\n",
    "                    # If a requested variable is missing, fill with NaN\n",
    "                    results[v] = [np.nan] * len(times)\n",
    "                else:\n",
    "                    # Convert to numeric (floats), keep NaN if parsing fails\n",
    "                    results[v] = [float(x) if x is not None else np.nan for x in vals]\n",
    "            df = pd.DataFrame(results, index=pd.to_datetime(times).date)\n",
    "            df.index.name = \"date\"\n",
    "            df = df[[v for v in daily_vars]]  # ensure column order\n",
    "            if use_cache:\n",
    "                _OPEN_METEO_CACHE[key] = df.copy()\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            last_exc = e\n",
    "            if attempt < max_retries:\n",
    "                time.sleep(0.5 * attempt)\n",
    "                continue\n",
    "            raise RuntimeError(f\"Failed fetching Open-Meteo archive: {e}\") from e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d36f72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "min_date = df_no_weather_code['Datum'].min()\n",
    "max_date = df_no_weather_code['Datum'].max()\n",
    "##min_date = \"2013-07-01\"\n",
    "##max_date = \"2014-07-30\"\n",
    "print (f\"Fetching weather data from {min_date} to {max_date}...\")\n",
    "lat_kiel, lon_kiel = 54.3233, 10.1228\n",
    "#daily_vars = ['precipitation_hours', 'sunshine_duration', 'rain_sum','temperature_2m_mean']\n",
    "daily_vars = ['sunshine_duration', 'temperature_2m_mean']\n",
    "df_extra_weather = fetch_open_meteo_daily_range(min_date, max_date, latitude=lat_kiel, longitude=lon_kiel, daily_vars=daily_vars)\n",
    "# print columns of df_extended_weather\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5c6d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_extra_weather['sunshine_hours'] = df_extra_weather['sunshine_duration'] / 3600.0\n",
    "#df_extended_weather.head()\n",
    "\n",
    "# print row count for df_extended_weather and df_no_weather_code\n",
    "print(f\"Extended Weather Data Rows: {len(df_extra_weather)}\")\n",
    "print(f\"Sales Data Rows: {len(df_no_weather_code)}\")\n",
    "# rename date to Datum\n",
    "df_extra_weather = df_extra_weather.rename_axis('Datum').reset_index()\n",
    "df_extra_weather.tail()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ac1577",
   "metadata": {},
   "source": [
    "### Extended Data Frames\n",
    "Its with extra weather data from open meteo api: df_merged_extended_weather\n",
    "with rolling 7 day average as well for umsatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841cb25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_weather_code['Datum'] = pd.to_datetime(df_no_weather_code['Datum'])\n",
    "df_extra_weather['Datum'] = pd.to_datetime(df_extra_weather['Datum'])\n",
    "\n",
    "df_merged_extended_weather = df_no_weather_code.merge(\n",
    "    df_extra_weather,\n",
    "    left_on='Datum',\n",
    "    right_on='Datum',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Merged DataFrame Rows: {len(df_merged_extended_weather)}\")\n",
    "print(f\"Merged DataFrame Shape: {df_merged_extended_weather.shape}\")\n",
    "\n",
    "#df_merged_extended_weather = df_merged_extended_weather.dropna(subset=[\"id_umsatz\"])\n",
    "#df_merged_extended_weather[\"id_umsatz\"] = df_merged_extended_weather[\"id_umsatz\"].astype(\"int64\")\n",
    "#df_merged_extended_weather = df_merged_extended_weather.dropna(subset=[\"KielerWoche_kiwo\"])\n",
    "#df_merged_extended_weather[\"KielerWoche_kiwo\"] = df_merged_extended_weather[\"KielerWoche_kiwo\"].astype(\"int64\")\n",
    "\n",
    "df_merged_extended_weather.head()\n",
    "\n",
    "plot_missing_heatmap(df_merged_extended_weather, 'Final Merged Data with Extended Weather')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce59d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read processed/df_holidays.csv\n",
    "df_holidays = pd.read_csv(\"../data/processed/df_holidays.csv\")\n",
    "# rename date to datum and date type to datetime\n",
    "df_holidays = df_holidays.rename(columns={\"date\": \"Datum\"})\n",
    "df_holidays['Datum'] = pd.to_datetime(df_holidays['Datum'])\n",
    "df_holidays.head()\n",
    "\n",
    "# merge df_merged_extended_weather with df_holidays on Datum\n",
    "df_merged_extended_weather_holidays = df_merged_extended_weather.merge(\n",
    "    df_holidays,\n",
    "    left_on='Datum',\n",
    "    right_on='Datum',\n",
    "    how='left'\n",
    ")\n",
    "print(f\"Merged DataFrame with Holidays Rows: {len(df_merged_extended_weather_holidays)}\")\n",
    "print(f\"Merged DataFrame with Holidays Shape: {df_merged_extended_weather_holidays.shape}\")\n",
    "df_merged_extended_weather_holidays.head()\n",
    "\n",
    "plot_missing_heatmap(df_merged_extended_weather_holidays, 'Final Merged Data with Extended Weather and Holidays')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dc00c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write complete df_merged_extended_weather to ../data/processed/df_extended_extras_with_test.csv\n",
    "df_merged_extended_weather_holidays.to_csv(\"../data/processed/df_extended_extras_with_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2bed2f",
   "metadata": {},
   "source": [
    "# Splitting above df_merged_extended_weather_holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3395685c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will split this raw data frame into train, validation, test in the next steps \n",
    "# training dataset from 01.07.2013 to 31.07.2017, a validation dataset from 01.08.2017 to 31.07.2018, and the test set from 01.08.2018 to 31.07.2019\n",
    "\n",
    "df_train_data_raw = df_merged_extended_weather_holidays[\n",
    "    (df_merged_extended_weather_holidays['Datum'] >= '2013-07-01') &\n",
    "    (df_merged_extended_weather_holidays['Datum'] <= '2017-07-31')\n",
    "]\n",
    "df_validation_data_raw = df_merged_extended_weather_holidays[\n",
    "    (df_merged_extended_weather_holidays['Datum'] >= '2017-08-01') &\n",
    "    (df_merged_extended_weather_holidays['Datum'] <= '2018-07-31')\n",
    "]\n",
    "df_test_data_raw = df_merged_extended_weather_holidays[\n",
    "    (df_merged_extended_weather_holidays['Datum'] >= '2018-08-01') &\n",
    "    (df_merged_extended_weather_holidays['Datum'] <= '2019-07-31')\n",
    "]\n",
    "\n",
    "#print shapes of the three dataframes\n",
    "print(f\"Train Data Raw Shape: {df_train_data_raw.shape}\")\n",
    "print(f\"Validation Data Raw Shape: {df_validation_data_raw.shape}\")\n",
    "print(f\"Test Data Raw Shape: {df_test_data_raw.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c70c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_missing_heatmap(df_train_data_raw, 'Train Data Raw')\n",
    "plot_missing_heatmap(df_validation_data_raw, 'Validation Data Raw')\n",
    "plot_missing_heatmap(df_test_data_raw, 'Test Data Raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4130ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write all three dataframes to csv files\n",
    "df_train_data_raw.to_csv(\"../data/processed/df_train_data_raw.csv\", index=False)\n",
    "df_validation_data_raw.to_csv(\"../data/processed/df_validation_data_raw.csv\", index=False)\n",
    "df_test_data_raw.to_csv(\"../data/processed/df_test_data_raw.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
